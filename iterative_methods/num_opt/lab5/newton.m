function [xk,fk, gradfk_norm, k, xseq] = newton(x0, f, gradf, Hessf, alpha, kmax, tolgrad)
% NEWTON METHOD
%   INPUTS:
% x0: a column vector of n elements representing the starting point for the optimization method;
% f: a function handle variable that, for each column vector x ∈ R^n, returns the value f(x), where f : R^n → R is the loss function that have to be minimized;
% gradf: a function handle variable that, for each column vector x ∈ R^n, returns the value ∇f(x) as a column vector, where ∇f : R^n → R^n is the gradient of f;
% Hessf: a function handle variable that, for each column vector x ∈ R^n, returns the matrix Hf (x) ∈ R^n×n, where Hf is the Hessian matrix of f;
% kmax: an integer scalar value characterizing the maximum number of iterations of the method;
% tolgrad: a real scalar value characterizing the tolerance with respect to the norm of the gradient in order to stop the method.
%   OUTPUTS:
% xk: the last vector xk ∈ R
% n computed by the optimization method before it stops;
% fk: the value f(xk);
% gradfk norm: the euclidean norm of ∇f(xk);
% k: index value of the last step executed by the optimization method before stopping;
% xseq: a matrix/vector in R^n×k such that each column j is the vector j-th vector xj ∈ R n generated by the iterations of the method.
xseq = zeros(length(x0), kmax);
xk = x0;
gradfk = gradf(xk);
k = 0;
gradfk_norm = norm(gradfk);

while k < kmax && gradfk_norm >= tolgrad
    pk = Hessf(xk)\(-gradfk);
    xk=xk+(alpha*pk);
    
    gradfk = gradf(xk);
    gradfk_norm = norm(gradfk);
    
    k = k + 1;
    
    % Store current xk in xseq
    xseq(:, k) = xk;
end
xseq = xseq(:, 1:k);
fk = f(xk);
end